{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 774,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03875968992248062,
      "grad_norm": 11.494925498962402,
      "learning_rate": 4.941860465116279e-05,
      "loss": 3.6701,
      "step": 10
    },
    {
      "epoch": 0.07751937984496124,
      "grad_norm": 11.536019325256348,
      "learning_rate": 4.877260981912145e-05,
      "loss": 2.6452,
      "step": 20
    },
    {
      "epoch": 0.11627906976744186,
      "grad_norm": 10.573549270629883,
      "learning_rate": 4.812661498708011e-05,
      "loss": 2.4615,
      "step": 30
    },
    {
      "epoch": 0.15503875968992248,
      "grad_norm": 12.559025764465332,
      "learning_rate": 4.7480620155038765e-05,
      "loss": 2.5205,
      "step": 40
    },
    {
      "epoch": 0.1937984496124031,
      "grad_norm": 9.401893615722656,
      "learning_rate": 4.6834625322997416e-05,
      "loss": 2.0917,
      "step": 50
    },
    {
      "epoch": 0.23255813953488372,
      "grad_norm": 11.778022766113281,
      "learning_rate": 4.6188630490956074e-05,
      "loss": 2.3838,
      "step": 60
    },
    {
      "epoch": 0.2713178294573643,
      "grad_norm": 11.615996360778809,
      "learning_rate": 4.554263565891473e-05,
      "loss": 2.0851,
      "step": 70
    },
    {
      "epoch": 0.31007751937984496,
      "grad_norm": 12.053035736083984,
      "learning_rate": 4.489664082687339e-05,
      "loss": 2.0915,
      "step": 80
    },
    {
      "epoch": 0.3488372093023256,
      "grad_norm": 14.501437187194824,
      "learning_rate": 4.425064599483205e-05,
      "loss": 1.8967,
      "step": 90
    },
    {
      "epoch": 0.3875968992248062,
      "grad_norm": 12.189470291137695,
      "learning_rate": 4.36046511627907e-05,
      "loss": 1.7283,
      "step": 100
    },
    {
      "epoch": 0.4263565891472868,
      "grad_norm": 13.706875801086426,
      "learning_rate": 4.2958656330749356e-05,
      "loss": 1.9381,
      "step": 110
    },
    {
      "epoch": 0.46511627906976744,
      "grad_norm": 13.404261589050293,
      "learning_rate": 4.2312661498708014e-05,
      "loss": 1.8008,
      "step": 120
    },
    {
      "epoch": 0.5038759689922481,
      "grad_norm": 12.621528625488281,
      "learning_rate": 4.166666666666667e-05,
      "loss": 1.7293,
      "step": 130
    },
    {
      "epoch": 0.5426356589147286,
      "grad_norm": 14.999897003173828,
      "learning_rate": 4.102067183462532e-05,
      "loss": 1.839,
      "step": 140
    },
    {
      "epoch": 0.5813953488372093,
      "grad_norm": 13.065555572509766,
      "learning_rate": 4.037467700258398e-05,
      "loss": 1.7558,
      "step": 150
    },
    {
      "epoch": 0.6201550387596899,
      "grad_norm": 13.956513404846191,
      "learning_rate": 3.972868217054264e-05,
      "loss": 1.8435,
      "step": 160
    },
    {
      "epoch": 0.6589147286821705,
      "grad_norm": 12.246381759643555,
      "learning_rate": 3.90826873385013e-05,
      "loss": 1.6839,
      "step": 170
    },
    {
      "epoch": 0.6976744186046512,
      "grad_norm": 13.01468563079834,
      "learning_rate": 3.843669250645995e-05,
      "loss": 1.5016,
      "step": 180
    },
    {
      "epoch": 0.7364341085271318,
      "grad_norm": 10.591852188110352,
      "learning_rate": 3.7790697674418606e-05,
      "loss": 1.8074,
      "step": 190
    },
    {
      "epoch": 0.7751937984496124,
      "grad_norm": 13.199572563171387,
      "learning_rate": 3.7144702842377264e-05,
      "loss": 1.6029,
      "step": 200
    },
    {
      "epoch": 0.813953488372093,
      "grad_norm": 12.733516693115234,
      "learning_rate": 3.649870801033592e-05,
      "loss": 1.534,
      "step": 210
    },
    {
      "epoch": 0.8527131782945736,
      "grad_norm": 12.985892295837402,
      "learning_rate": 3.585271317829458e-05,
      "loss": 1.6738,
      "step": 220
    },
    {
      "epoch": 0.8914728682170543,
      "grad_norm": 12.046661376953125,
      "learning_rate": 3.520671834625323e-05,
      "loss": 1.2755,
      "step": 230
    },
    {
      "epoch": 0.9302325581395349,
      "grad_norm": 15.02818775177002,
      "learning_rate": 3.456072351421189e-05,
      "loss": 1.4786,
      "step": 240
    },
    {
      "epoch": 0.9689922480620154,
      "grad_norm": 14.614781379699707,
      "learning_rate": 3.3914728682170546e-05,
      "loss": 1.414,
      "step": 250
    },
    {
      "epoch": 1.0077519379844961,
      "grad_norm": 13.380239486694336,
      "learning_rate": 3.3268733850129204e-05,
      "loss": 1.6527,
      "step": 260
    },
    {
      "epoch": 1.0465116279069768,
      "grad_norm": 12.898655891418457,
      "learning_rate": 3.2622739018087855e-05,
      "loss": 1.3989,
      "step": 270
    },
    {
      "epoch": 1.0852713178294573,
      "grad_norm": 13.305195808410645,
      "learning_rate": 3.197674418604651e-05,
      "loss": 1.6804,
      "step": 280
    },
    {
      "epoch": 1.124031007751938,
      "grad_norm": 14.404677391052246,
      "learning_rate": 3.133074935400517e-05,
      "loss": 1.5476,
      "step": 290
    },
    {
      "epoch": 1.1627906976744187,
      "grad_norm": 13.597051620483398,
      "learning_rate": 3.068475452196383e-05,
      "loss": 1.4309,
      "step": 300
    },
    {
      "epoch": 1.2015503875968991,
      "grad_norm": 15.521419525146484,
      "learning_rate": 3.003875968992248e-05,
      "loss": 1.5791,
      "step": 310
    },
    {
      "epoch": 1.2403100775193798,
      "grad_norm": 16.791488647460938,
      "learning_rate": 2.9392764857881138e-05,
      "loss": 1.3897,
      "step": 320
    },
    {
      "epoch": 1.2790697674418605,
      "grad_norm": 13.014751434326172,
      "learning_rate": 2.8746770025839792e-05,
      "loss": 1.4624,
      "step": 330
    },
    {
      "epoch": 1.3178294573643412,
      "grad_norm": 13.044551849365234,
      "learning_rate": 2.810077519379845e-05,
      "loss": 1.2116,
      "step": 340
    },
    {
      "epoch": 1.3565891472868217,
      "grad_norm": 13.61237907409668,
      "learning_rate": 2.745478036175711e-05,
      "loss": 1.4126,
      "step": 350
    },
    {
      "epoch": 1.3953488372093024,
      "grad_norm": 14.73280143737793,
      "learning_rate": 2.6808785529715763e-05,
      "loss": 1.3805,
      "step": 360
    },
    {
      "epoch": 1.4341085271317828,
      "grad_norm": 12.775725364685059,
      "learning_rate": 2.616279069767442e-05,
      "loss": 1.3697,
      "step": 370
    },
    {
      "epoch": 1.4728682170542635,
      "grad_norm": 12.890605926513672,
      "learning_rate": 2.5516795865633075e-05,
      "loss": 1.2724,
      "step": 380
    },
    {
      "epoch": 1.5116279069767442,
      "grad_norm": 12.276010513305664,
      "learning_rate": 2.4870801033591733e-05,
      "loss": 1.3863,
      "step": 390
    },
    {
      "epoch": 1.550387596899225,
      "grad_norm": 14.41694164276123,
      "learning_rate": 2.4224806201550387e-05,
      "loss": 1.2808,
      "step": 400
    },
    {
      "epoch": 1.5891472868217056,
      "grad_norm": 12.828203201293945,
      "learning_rate": 2.3578811369509045e-05,
      "loss": 1.1937,
      "step": 410
    },
    {
      "epoch": 1.627906976744186,
      "grad_norm": 15.487648010253906,
      "learning_rate": 2.2932816537467703e-05,
      "loss": 1.258,
      "step": 420
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 12.501500129699707,
      "learning_rate": 2.2286821705426357e-05,
      "loss": 1.2488,
      "step": 430
    },
    {
      "epoch": 1.7054263565891472,
      "grad_norm": 13.284682273864746,
      "learning_rate": 2.1640826873385015e-05,
      "loss": 1.4335,
      "step": 440
    },
    {
      "epoch": 1.744186046511628,
      "grad_norm": 13.322102546691895,
      "learning_rate": 2.099483204134367e-05,
      "loss": 1.2094,
      "step": 450
    },
    {
      "epoch": 1.7829457364341086,
      "grad_norm": 12.040922164916992,
      "learning_rate": 2.0348837209302328e-05,
      "loss": 1.2805,
      "step": 460
    },
    {
      "epoch": 1.8217054263565893,
      "grad_norm": 11.766929626464844,
      "learning_rate": 1.9702842377260982e-05,
      "loss": 1.3745,
      "step": 470
    },
    {
      "epoch": 1.8604651162790697,
      "grad_norm": 13.06673812866211,
      "learning_rate": 1.905684754521964e-05,
      "loss": 1.249,
      "step": 480
    },
    {
      "epoch": 1.8992248062015504,
      "grad_norm": 12.73422622680664,
      "learning_rate": 1.8410852713178295e-05,
      "loss": 1.3046,
      "step": 490
    },
    {
      "epoch": 1.937984496124031,
      "grad_norm": 12.665433883666992,
      "learning_rate": 1.7764857881136952e-05,
      "loss": 1.1265,
      "step": 500
    },
    {
      "epoch": 1.9767441860465116,
      "grad_norm": 13.11125373840332,
      "learning_rate": 1.7118863049095607e-05,
      "loss": 1.2583,
      "step": 510
    },
    {
      "epoch": 2.0155038759689923,
      "grad_norm": 16.429880142211914,
      "learning_rate": 1.647286821705426e-05,
      "loss": 1.2631,
      "step": 520
    },
    {
      "epoch": 2.054263565891473,
      "grad_norm": 13.774279594421387,
      "learning_rate": 1.5826873385012923e-05,
      "loss": 1.2413,
      "step": 530
    },
    {
      "epoch": 2.0930232558139537,
      "grad_norm": 14.238810539245605,
      "learning_rate": 1.5180878552971577e-05,
      "loss": 1.3766,
      "step": 540
    },
    {
      "epoch": 2.1317829457364343,
      "grad_norm": 11.347545623779297,
      "learning_rate": 1.4534883720930233e-05,
      "loss": 1.2711,
      "step": 550
    },
    {
      "epoch": 2.1705426356589146,
      "grad_norm": 14.1196928024292,
      "learning_rate": 1.388888888888889e-05,
      "loss": 1.3191,
      "step": 560
    },
    {
      "epoch": 2.2093023255813953,
      "grad_norm": 12.307211875915527,
      "learning_rate": 1.3242894056847546e-05,
      "loss": 1.1938,
      "step": 570
    },
    {
      "epoch": 2.248062015503876,
      "grad_norm": 14.010497093200684,
      "learning_rate": 1.2596899224806202e-05,
      "loss": 1.1975,
      "step": 580
    },
    {
      "epoch": 2.2868217054263567,
      "grad_norm": 14.385970115661621,
      "learning_rate": 1.1950904392764858e-05,
      "loss": 1.2853,
      "step": 590
    },
    {
      "epoch": 2.3255813953488373,
      "grad_norm": 13.608287811279297,
      "learning_rate": 1.1304909560723514e-05,
      "loss": 1.3588,
      "step": 600
    },
    {
      "epoch": 2.3643410852713176,
      "grad_norm": 11.195499420166016,
      "learning_rate": 1.065891472868217e-05,
      "loss": 1.1798,
      "step": 610
    },
    {
      "epoch": 2.4031007751937983,
      "grad_norm": 12.431131362915039,
      "learning_rate": 1.0012919896640828e-05,
      "loss": 1.2321,
      "step": 620
    },
    {
      "epoch": 2.441860465116279,
      "grad_norm": 11.663527488708496,
      "learning_rate": 9.366925064599485e-06,
      "loss": 1.0377,
      "step": 630
    },
    {
      "epoch": 2.4806201550387597,
      "grad_norm": 13.379507064819336,
      "learning_rate": 8.72093023255814e-06,
      "loss": 1.0715,
      "step": 640
    },
    {
      "epoch": 2.5193798449612403,
      "grad_norm": 12.230171203613281,
      "learning_rate": 8.074935400516797e-06,
      "loss": 1.299,
      "step": 650
    },
    {
      "epoch": 2.558139534883721,
      "grad_norm": 12.721207618713379,
      "learning_rate": 7.428940568475452e-06,
      "loss": 1.0502,
      "step": 660
    },
    {
      "epoch": 2.5968992248062017,
      "grad_norm": 10.259293556213379,
      "learning_rate": 6.782945736434108e-06,
      "loss": 1.1613,
      "step": 670
    },
    {
      "epoch": 2.6356589147286824,
      "grad_norm": 14.52176570892334,
      "learning_rate": 6.1369509043927654e-06,
      "loss": 1.4597,
      "step": 680
    },
    {
      "epoch": 2.6744186046511627,
      "grad_norm": 11.683006286621094,
      "learning_rate": 5.490956072351422e-06,
      "loss": 1.173,
      "step": 690
    },
    {
      "epoch": 2.7131782945736433,
      "grad_norm": 12.679608345031738,
      "learning_rate": 4.844961240310078e-06,
      "loss": 1.1484,
      "step": 700
    },
    {
      "epoch": 2.751937984496124,
      "grad_norm": 14.261682510375977,
      "learning_rate": 4.198966408268734e-06,
      "loss": 1.1352,
      "step": 710
    },
    {
      "epoch": 2.7906976744186047,
      "grad_norm": 12.030043601989746,
      "learning_rate": 3.55297157622739e-06,
      "loss": 1.0505,
      "step": 720
    },
    {
      "epoch": 2.8294573643410854,
      "grad_norm": 13.737723350524902,
      "learning_rate": 2.9069767441860468e-06,
      "loss": 1.0786,
      "step": 730
    },
    {
      "epoch": 2.8682170542635657,
      "grad_norm": 12.702534675598145,
      "learning_rate": 2.260981912144703e-06,
      "loss": 1.3355,
      "step": 740
    },
    {
      "epoch": 2.9069767441860463,
      "grad_norm": 11.93367862701416,
      "learning_rate": 1.6149870801033591e-06,
      "loss": 1.1908,
      "step": 750
    },
    {
      "epoch": 2.945736434108527,
      "grad_norm": 12.541996955871582,
      "learning_rate": 9.689922480620155e-07,
      "loss": 1.1757,
      "step": 760
    },
    {
      "epoch": 2.9844961240310077,
      "grad_norm": 13.453625679016113,
      "learning_rate": 3.2299741602067186e-07,
      "loss": 1.1884,
      "step": 770
    }
  ],
  "logging_steps": 10,
  "max_steps": 774,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 101121842479104.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
